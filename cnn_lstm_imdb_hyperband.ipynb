{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn-lstm-imdb-hyperband.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cXRRPjTk5gv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea36a465-c6cc-468f-8641-22705e034919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import sys\n",
        "import os\n",
        "prefix = '/content/gdrive/My Drive/'\n",
        "# modify \"customized_path_to_your_homework\" here to where you uploaded your homework\n",
        "customized_path_to_your_homework = 'IDLSProject-main'\n",
        "sys_path = os.path.join(prefix, customized_path_to_your_homework)\n",
        "sys.path.append(sys_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/gdrive/My Drive/IDLSProject-main-2'"
      ],
      "metadata": {
        "id": "b8m8aQCL5pFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f919a514-7d6a-45d4-9b86-80d0c83093ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/IDLSProject-main-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_dir = './data/pytorch'\n",
        "with open(os.path.join(data_dir, 'word_dict_qrnn.pkl'), \"rb\") as f:\n",
        "    word_dict = pickle.load(f)\n",
        "\n",
        "train = pd.read_csv(os.path.join(data_dir, 'train_qrnn.csv'), header=None, names=None)\n",
        "test_sample = pd.read_csv(os.path.join(data_dir, 'test_qrnn.csv'), header=None, names=None)\n",
        "\n",
        "test, val = train_test_split(test_sample, test_size=0.5)\n",
        "train.shape, test.shape, val.shape\n",
        "\n",
        "\n",
        "# Turn the input pandas dataframe into tensors\n",
        "train_y = torch.from_numpy(train[[0]].values).float()\n",
        "train_X = torch.from_numpy(train.drop([0, 1], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "train_ds = torch.utils.data.TensorDataset(train_X, train_y)\n",
        "# Build the dataloader\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=50)\n",
        "\n",
        "######val data\n",
        "# Turn the input pandas dataframe into tensors\n",
        "val_y = torch.from_numpy(val[[0]].values).float()\n",
        "val_X = torch.from_numpy(val.drop([0, 1], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "val_ds = torch.utils.data.TensorDataset(val_X, val_y)\n",
        "# Build the dataloader\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=50)\n",
        "\n",
        "\n",
        "#### Test data\n",
        "# Turn the input pandas dataframe into tensors\n",
        "test_y = torch.from_numpy(test[[0]].values).float()\n",
        "test_X = torch.from_numpy(test.drop([0, 1], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "test_ds = torch.utils.data.TensorDataset(test_X, test_y)\n",
        "# Build the dataloader\n",
        "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=50)\n",
        "print(test_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h8gAUJ_5q-2",
        "outputId": "f1920b67-151c-4832-b9d5-7bd0ace109da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs = 10\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "seq_len = 500\n",
        "dropout = 0.5\n",
        "filter_size = 100\n",
        "vocab_size = 5000\n",
        "embed_dims = 32\n",
        "hidden_size = 100\n",
        "kernel_size = [3,4,5]"
      ],
      "metadata": {
        "id": "DT-bPvic542j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './cnn-lstm-imdb-hyperband-trails/' # The folder we will use for storing data\n",
        "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "filename = \"\"\n",
        "def write_to_csv(trail_num, epochs, train_loss, train_acc, val_loss, val_acc, time_train):\n",
        "    global filename\n",
        "    filename = \"./cnn-lstm-imdb-hyperband-trails/\"+str(trail_num)+\".csv\"\n",
        "    epoch = [i for i in range(epochs)]\n",
        "    df_metrics = pd.DataFrame(list(zip(epoch, train_loss, train_acc, val_loss, val_acc, time_train)), columns =['Epoch', 'train_loss', 'train_acc', 'val_loss', 'val_acc', 'train_time'])\n",
        "    df_metrics.to_csv(filename, index=False)    \n",
        "    \n",
        "def append_to_csv(epochs, accuracy):\n",
        "    acc = [accuracy for i in range(epochs)]\n",
        "    df_csv = pd.read_csv(filename)\n",
        "    df_csv['Test_Accuracy']  = acc\n",
        "    df_csv.to_csv(filename, index=False)"
      ],
      "metadata": {
        "id": "OtBMgWdy58jp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "class Combine(nn.Module):\n",
        "    def __init__(self,trial,vocab_size, embed_size, filter_size, kernel_size, dropout, seq_len,hidden_size):\n",
        "        super(Combine, self).__init__()\n",
        "        dropout = trial.suggest_uniform(\"dropout\",0.1, 0.6)\n",
        "        hidden_size = trial.suggest_int(\"hidden_size\",32,256)\n",
        "        embed_size = trial.suggest_int(\"embed_size\",16,128)\n",
        "        n_filters = trial.suggest_int(\"n_filters\",50,200)\n",
        "        self.embedding = torch.nn.Embedding(vocab_size,embed_size)\n",
        "        self.conv1 = torch.nn.Conv2d(1,filter_size,kernel_size=[kernel_size[0],embed_size])\n",
        "        self.conv2 = torch.nn.Conv2d(1,filter_size,kernel_size=[kernel_size[1],embed_size])\n",
        "        self.conv3 = torch.nn.Conv2d(1,filter_size,kernel_size=[kernel_size[2],embed_size])\n",
        "        self.mp1 = torch.nn.MaxPool1d(seq_len+1-kernel_size[0])\n",
        "        self.mp2 = torch.nn.MaxPool1d(seq_len+1-kernel_size[1])\n",
        "        self.mp3 = torch.nn.MaxPool1d(seq_len+1-kernel_size[2])\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        #self.cnn = CNN(vocab_size, embed_size, filter_size, kernel_size, dropout, seq_len)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=3*filter_size, \n",
        "            hidden_size=hidden_size, \n",
        "            num_layers=1,\n",
        "            batch_first=True)\n",
        "        self.dense = nn.Linear(hidden_size,1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.word_dict = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        embed_input = self.embedding(x)\n",
        "        embed_input.unsqueeze_(1)\n",
        "        x1 = torch.tanh(self.dropout(self.conv1(embed_input))).squeeze(3)\n",
        "        # print(\"x1\")\n",
        "        # print(x1.shape)\n",
        "        x2 = torch.tanh(self.dropout(self.conv2(embed_input))).squeeze(3)\n",
        "        x3 = torch.tanh(self.dropout(self.conv3(embed_input))).squeeze(3)\n",
        "        f1 = self.mp1(x1).squeeze(2)\n",
        "        # print(\"f1\")\n",
        "        # print(f1.shape)\n",
        "        f2 = self.mp2(x2).squeeze(2)\n",
        "        f3 = self.mp3(x3).squeeze(2)\n",
        "        c_out = torch.cat([f1,f2,f3],dim=1)\n",
        "        r_in = c_out.view(batch_size,1, -1)\n",
        "        lstm_out, _ = self.lstm(r_in)\n",
        "        out = self.dense(lstm_out[:, -1, :])\n",
        "        return self.sig(out)"
      ],
      "metadata": {
        "id": "lOi5VnD-6Afi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np \n",
        "def objective(trial):\n",
        "  optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-0)\n",
        "  #momentum = trial.suggest_uniform(\"momentum\", 0.0, 1.0)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = Combine(trial,vocab_size, embed_dims, filter_size, kernel_size, dropout, seq_len,hidden_size).to(device).to(device)\n",
        "  trial.set_user_attr(key=\"best_model\", value=model)\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "  loss_fn = torch.nn.BCELoss()\n",
        "  train_loss_epoch = []\n",
        "  train_acc_epoch = []\n",
        "  val_loss_epoch = []\n",
        "  val_accuracy_epoch = []\n",
        "  time_train = []\n",
        "  val_acc = 0\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    train_acc = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for batch in train_dl:         \n",
        "      batch_X, batch_y = batch\n",
        "      batch_X = batch_X.to(device)\n",
        "      batch_y = batch_y.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      prediction = model(batch_X)\n",
        "      loss = loss_fn(prediction, batch_y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      result = np.round(prediction.detach().cpu())\n",
        "      total_loss += loss.data.item()\n",
        "      total += batch_y.size(0)\n",
        "      correct += (result == batch_y.cpu()).sum().item()\n",
        "      train_acc = correct/total\n",
        "    train_loss_epoch.append(np.round(total_loss / len(train_dl), 3))\n",
        "    train_acc_epoch.append(np.round(train_acc*100,3))\n",
        "    print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_dl)))\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      val_loss = []\n",
        "      for inputs, labels in val_dl:\n",
        "        inputs_val, labels_val = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        prediction = model(inputs_val)\n",
        "        loss = loss_fn(prediction, labels_val)\n",
        "        val_loss.append(loss.item())\n",
        "        result = np.round(prediction.cpu())\n",
        "        total += labels_val.size(0)\n",
        "        correct += (result == labels_val.cpu()).sum().item()\n",
        "      val_accuracy_epoch.append(np.round((correct/total)*100, 3))\n",
        "      val_loss_epoch.append(np.round(np.mean(val_loss),3))\n",
        "      end = time.time() - start\n",
        "      print(\"Val Loss: {:.3f}\".format(np.mean(val_loss)), \"\\tVal Acc: {:.3f}\".format(correct/total))\n",
        "      time_train.append(np.round(end,3))\n",
        "      val_acc = np.round((correct/total)*100, 3)\n",
        "    write_to_csv(trial.number,epochs, train_loss_epoch, train_acc_epoch, val_loss_epoch, val_accuracy_epoch, time_train)\n",
        "    return val_acc\n",
        "\n",
        "\n",
        "def test(model, test_dl, epochs):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "#     results = []\n",
        "#     labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dl:         \n",
        "            batch_X, batch_y = batch\n",
        "            batch_X = batch_X.to(device)\n",
        "            prediction = model(batch_X)\n",
        "            result = np.round(prediction.cpu())\n",
        "#             results.extend(list(result.numpy()))\n",
        "#             labels.extend(list(batch_y.numpy()))\n",
        "            total += batch_y.size(0)\n",
        "            correct += (result == batch_y).sum().item()\n",
        "    acc = correct/total\n",
        "    print(\"Accuracy:\", (correct/total)*100)"
      ],
      "metadata": {
        "id": "yhUhQgON6Ft4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def callback(study, trial):\n",
        "    if study.best_trial.number == trial.number:\n",
        "        study.set_user_attr(key=\"best_model\", value=trial.user_attrs[\"best_model\"])"
      ],
      "metadata": {
        "id": "P1THPr2-9CNZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oKeaSUz_olk",
        "outputId": "0fa1f844-4c75-4eb3-a908-2b37c1271f46"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.7)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.10.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (6.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.36)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.8)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.2.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.7.1)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.9.0)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (0.5.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.4.1)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "SMRMuN_C_5lR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "study = optuna.create_study(direction=\"maximize\",pruner=optuna.pruners.HyperbandPruner(\n",
        "        min_resource=1, max_resource=epochs, reduction_factor=3\n",
        "    ),)\n",
        "study.optimize(objective, n_trials=5,callbacks=[callback])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "best_model=study.user_attrs[\"best_model\"]\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "  print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "PlVqr1IX9FJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfb22a0-c115-45a1-a3fe-e0bb69af9d04"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-05-14 19:19:41,946]\u001b[0m A new study created in memory with name: no-name-d07ec7e0-5ea4-48c7-ab91-69cfc99a2338\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, BCELoss: 0.582429970651865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-05-14 19:19:55,928]\u001b[0m Trial 0 finished with value: 79.12 and parameters: {'optimizer': 'Adam', 'lr': 9.296672919567677e-05, 'dropout': 0.17128277945653755, 'hidden_size': 181, 'embed_size': 42, 'n_filters': 188}. Best is trial 0 with value: 79.12.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.447 \tVal Acc: 0.791\n",
            "Epoch: 0, BCELoss: 0.5636818692584833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-05-14 19:20:04,282]\u001b[0m Trial 1 finished with value: 81.89 and parameters: {'optimizer': 'SGD', 'lr': 0.17408070875658735, 'dropout': 0.18453676364573948, 'hidden_size': 139, 'embed_size': 50, 'n_filters': 92}. Best is trial 1 with value: 81.89.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.409 \tVal Acc: 0.819\n",
            "Epoch: 0, BCELoss: 0.5708656214674314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-05-14 19:20:13,873]\u001b[0m Trial 2 finished with value: 81.86 and parameters: {'optimizer': 'RMSprop', 'lr': 0.00022447873765580885, 'dropout': 0.2890308613959798, 'hidden_size': 256, 'embed_size': 85, 'n_filters': 133}. Best is trial 1 with value: 81.89.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.410 \tVal Acc: 0.819\n",
            "Epoch: 0, BCELoss: 0.5562615537643433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-05-14 19:20:22,839]\u001b[0m Trial 3 finished with value: 83.36 and parameters: {'optimizer': 'RMSprop', 'lr': 0.12360115688111847, 'dropout': 0.19480660133684466, 'hidden_size': 69, 'embed_size': 95, 'n_filters': 51}. Best is trial 3 with value: 83.36.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.387 \tVal Acc: 0.834\n",
            "Epoch: 0, BCELoss: 0.6775266034404437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-05-14 19:20:32,206]\u001b[0m Trial 4 finished with value: 76.7 and parameters: {'optimizer': 'SGD', 'lr': 3.984785303484685e-05, 'dropout': 0.5476721857137145, 'hidden_size': 192, 'embed_size': 88, 'n_filters': 91}. Best is trial 3 with value: 83.36.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.513 \tVal Acc: 0.767\n",
            "Study statistics: \n",
            "  Number of finished trials:  5\n",
            "  Number of complete trials:  5\n",
            "Best trial:\n",
            "  Value:  83.36\n",
            "  Params: \n",
            "    optimizer: RMSprop\n",
            "    lr: 0.12360115688111847\n",
            "    dropout: 0.19480660133684466\n",
            "    hidden_size: 69\n",
            "    embed_size: 95\n",
            "    n_filters: 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(best_model.state_dict(),\"./cnn-lstm-imdb-hyperband.pth\")"
      ],
      "metadata": {
        "id": "SczpZ5Ab6Kb_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(best_model, test_dl, epochs)"
      ],
      "metadata": {
        "id": "cSg_6YND9Jus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63901699-2e5c-410a-94fc-3b2481e094a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:762: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:926.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 83.95\n"
          ]
        }
      ]
    }
  ]
}