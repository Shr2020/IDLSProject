{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm-cnn-imdb.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wke9GGY-Vhi9",
        "outputId": "38a8f7e3-331f-4f81-ea49-7c5c4c4e59a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import sys\n",
        "import os\n",
        "prefix = '/content/gdrive/My Drive/'\n",
        "# modify \"customized_path_to_your_homework\" here to where you uploaded your homework\n",
        "customized_path_to_your_homework = 'IDLSProject-main-2'\n",
        "sys_path = os.path.join(prefix, customized_path_to_your_homework)\n",
        "sys.path.append(sys_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/gdrive/My Drive/IDLSProject-main-2'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbpmqPd2Vzqt",
        "outputId": "c4aaf00f-0754-4b5c-d214-f7333926f993"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/IDLSProject-main-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_dir = './data/pytorch'\n",
        "with open(os.path.join(data_dir, 'word_dict_qrnn.pkl'), \"rb\") as f:\n",
        "    word_dict = pickle.load(f)\n",
        "\n",
        "train = pd.read_csv(os.path.join(data_dir, 'train_qrnn.csv'), header=None, names=None)\n",
        "test_sample = pd.read_csv(os.path.join(data_dir, 'test_qrnn.csv'), header=None, names=None)\n",
        "\n",
        "test, val = train_test_split(test_sample, test_size=0.5)\n",
        "train.shape, test.shape, val.shape\n",
        "\n",
        "\n",
        "# Turn the input pandas dataframe into tensors\n",
        "train_y = torch.from_numpy(train[[0]].values).float()\n",
        "train_X = torch.from_numpy(train.drop([0, 1], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "train_ds = torch.utils.data.TensorDataset(train_X, train_y)\n",
        "# Build the dataloader\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=50)\n",
        "\n",
        "######val data\n",
        "# Turn the input pandas dataframe into tensors\n",
        "val_y = torch.from_numpy(val[[0]].values).float()\n",
        "val_X = torch.from_numpy(val.drop([0, 1], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "val_ds = torch.utils.data.TensorDataset(val_X, val_y)\n",
        "# Build the dataloader\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=50)\n",
        "\n",
        "\n",
        "#### Test data\n",
        "# Turn the input pandas dataframe into tensors\n",
        "test_y = torch.from_numpy(test[[0]].values).float()\n",
        "test_X = torch.from_numpy(test.drop([0, 1], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "test_ds = torch.utils.data.TensorDataset(test_X, test_y)\n",
        "# Build the dataloader\n",
        "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=50)\n",
        "print(test_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJUW-ZwSV9sY",
        "outputId": "f08d8f4e-fca6-4a9c-d4da-1ceb1960e3f5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs = 10\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "seq_len = 500\n",
        "dropout = 0.5\n",
        "filter_size = 100\n",
        "vocab_size = 5000\n",
        "embed_dims = 32\n",
        "hidden_size = 100\n",
        "kernel_size = [3,4,5]"
      ],
      "metadata": {
        "id": "AfhXAHD_WEh8"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write to file.\n",
        "filename = \"lstm-cnn-imdb.csv\"\n",
        "def write_to_csv(epochs, train_loss, train_acc, val_loss, val_acc, time_train):\n",
        "    epoch = [i for i in range(epochs)]\n",
        "    df_metrics = pd.DataFrame(list(zip(epoch, train_loss, train_acc, val_loss, val_acc, time_train)), columns =['Epoch', 'train_loss', 'train_acc', 'val_loss', 'val_acc', 'train_time'])\n",
        "    df_metrics.to_csv(filename, index=False)\n",
        "    \n",
        "def append_to_csv(epochs, accuracy):\n",
        "    acc = [accuracy for i in range(epochs)]\n",
        "    df_csv = pd.read_csv(filename)\n",
        "    df_csv['Test_Accuracy']  = acc\n",
        "    df_csv.to_csv(filename, index=False)"
      ],
      "metadata": {
        "id": "_Yp0LBkIWHEw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "class Combine(nn.Module):\n",
        "    def __init__(self,vocab_size, embed_size, filter_size, kernel_size, dropout, seq_len,hidden_size):\n",
        "        super(Combine, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(input_size = embed_size, hidden_size = hidden_size,num_layers=1,batch_first=True)\n",
        "        self.conv1 = torch.nn.Conv2d(1,filter_size,kernel_size=[kernel_size[0],hidden_size])\n",
        "        self.conv2 = torch.nn.Conv2d(1,filter_size,kernel_size=[kernel_size[1],hidden_size])\n",
        "        self.conv3 = torch.nn.Conv2d(1,filter_size,kernel_size=[kernel_size[2],hidden_size])\n",
        "        self.mp1 = torch.nn.MaxPool1d(seq_len+1-kernel_size[0])\n",
        "        self.mp2 = torch.nn.MaxPool1d(seq_len+1-kernel_size[1])\n",
        "        self.mp3 = torch.nn.MaxPool1d(seq_len+1-kernel_size[2])\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.dense = nn.Linear(3*hidden_size,1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.word_dict = None\n",
        "\n",
        "\n",
        "    def forward(self, x,target):\n",
        "    \n",
        "        embeds = self.embedding(x)\n",
        "        # print('embedds')\n",
        "        # print(embeds.shape)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        # print('lstm output')\n",
        "        # print(lstm_out.shape)\n",
        "        lstm_out = lstm_out.clone()\n",
        "        lstm_out.unsqueeze_(1)\n",
        "        # print('lstm output after')\n",
        "        # print(lstm_out.shape)\n",
        "        x1 = torch.tanh(self.dropout(self.conv1(lstm_out))).squeeze(3)\n",
        "        x2 = torch.tanh(self.dropout(self.conv2(lstm_out))).squeeze(3)\n",
        "        x3 = torch.tanh(self.dropout(self.conv3(lstm_out))).squeeze(3)\n",
        "        # print('x1.shape')\n",
        "        # print(x1.shape)\n",
        "        f1 = self.mp1(x1).squeeze(2)\n",
        "        # print('f1')\n",
        "        # print(f1.shape)\n",
        "        f2 = self.mp2(x2).squeeze(2)\n",
        "        # print('f2')\n",
        "        # print(f2.shape)\n",
        "        f3 = self.mp3(x3).squeeze(2)\n",
        "        # print('f3')\n",
        "        # print(f3.shape)\n",
        "        hidden = torch.cat([f1,f2,f3],dim=1)\n",
        "        logits = self.dense(hidden)\n",
        "        prediction = torch.sigmoid(logits)\n",
        "        target = target.view([-1,1])\n",
        "        correct_pred = torch.eq(torch.round(prediction).type(target.type()),target)\n",
        "        accuracy = torch.sum(correct_pred)\n",
        "        return prediction, accuracy\n",
        "        # return self.sig(out)"
      ],
      "metadata": {
        "id": "REymrbOqWO-G"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs = 10\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "embed_size = 300\n",
        "seq_len = 500\n",
        "dropout = 0.5\n",
        "filter_size = 100\n",
        "vocab_size = 5000\n",
        "embed_dims = 32\n",
        "kernel_size = [3,4,5]\n",
        "model = Combine(vocab_size, embed_dims, filter_size, kernel_size, dropout, seq_len,hidden_size).to(device).to(device)"
      ],
      "metadata": {
        "id": "xBOE2V9Co-1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "criterion = torch.nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "counter = 0\n",
        "CNN_acc = []\n",
        "CNN_valacc = []\n",
        "train_loss_epoch = []\n",
        "train_acc_epoch = []\n",
        "val_loss_epoch = []\n",
        "val_acc_epoch = []\n",
        "time_epoch = []\n",
        "model.train()\n",
        "for e in range(epochs):\n",
        "    start_time = time.time()\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    for inputs, labels in train_dl:\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        model.zero_grad()\n",
        "        logits, accuracy = model(inputs,labels)\n",
        "        loss = criterion(logits,labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        train_loss.append(loss.item())\n",
        "        train_acc.append(accuracy.item()*100/batch_size)\n",
        "        if counter%100==0:\n",
        "            print(\"Epoch: {}/{}\".format(e,epochs),\n",
        "                         \"\\tIteration: {}\".format(counter),\n",
        "                         \"\\t\\tTrain Loss: {:.3f}\".format(loss.item()),\n",
        "                         \"\\tTrain Accuracy: {:.2f}\".format(accuracy.item()*100/batch_size))\n",
        "            CNN_acc.append(accuracy.item()*100/batch_size)\n",
        "        counter += 1\n",
        "    train_loss_epoch.append(np.round(np.mean(train_loss), 3))\n",
        "    train_acc_epoch.append(np.round(np.mean(train_acc), 3))\n",
        "    print(\"\\tTrain Loss: {:.3f}\".format(np.mean(train_loss)), \"\\tTrain Acc: {:.3f}\".format(np.mean(train_acc)))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        val_acc = []\n",
        "        val_loss = []\n",
        "        for inputs, labels in val_dl:\n",
        "            inputs_val, labels_val = inputs.cuda(), labels.cuda()\n",
        "            logits_val,accuracy_val = model(inputs_val,labels_val)\n",
        "            loss_val = criterion(logits_val,labels_val.float())\n",
        "            val_acc.append(accuracy_val.item()*100/batch_size)\n",
        "            val_loss.append(loss_val.item())\n",
        "        val_loss_epoch.append(np.round(np.mean(val_loss), 3))\n",
        "        val_acc_epoch.append(np.round(np.mean(val_acc), 3))\n",
        "        print(\"\\t\\tVal Loss: {:.3f}\".format(np.mean(val_loss)), \"\\t\\tVal Acc: {:.3f}\".format(np.mean(val_acc)))\n",
        "        CNN_valacc.append(np.mean(val_acc))\n",
        "        model.train()\n",
        "        \n",
        "    end_time = time.time()-start_time\n",
        "    print(\"Time to train epoch: {0} s\".format(end_time))\n",
        "    time_epoch.append(np.round(end_time, 3))\n",
        "    \n",
        "write_to_csv(epochs, train_loss_epoch, train_acc_epoch, val_loss_epoch, val_acc_epoch, time_epoch)\n",
        "    \n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_acc = []\n",
        "    test_loss = []\n",
        "    for inputs, labels in test_dl:\n",
        "        input_test, labels_test = inputs.cuda(), labels.cuda()\n",
        "        logits_test,accuracy_test = model(input_test,labels_test)\n",
        "        loss_test = criterion(logits_test,labels_test.float())\n",
        "        test_acc.append(accuracy_test.item()*100/batch_size)\n",
        "        test_loss.append(loss_test.item())\n",
        "    print(\"Test Loss: {:.3f}\".format(np.mean(test_loss)), \"\\tTest Acc: {:.3f}\".format(np.mean(test_acc)))\n",
        "    append_to_csv(epochs, np.round(np.mean(test_acc),3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ2NQqkBWV_z",
        "outputId": "8806d17d-b01b-4bf5-ba96-469b0e0554ba"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/10 \tIteration: 0 \t\tTrain Loss: 0.696 \tTrain Accuracy: 50.00\n",
            "Epoch: 0/10 \tIteration: 100 \t\tTrain Loss: 0.646 \tTrain Accuracy: 58.00\n",
            "Epoch: 0/10 \tIteration: 200 \t\tTrain Loss: 0.499 \tTrain Accuracy: 80.00\n",
            "Epoch: 0/10 \tIteration: 300 \t\tTrain Loss: 0.385 \tTrain Accuracy: 80.00\n",
            "Epoch: 0/10 \tIteration: 400 \t\tTrain Loss: 0.464 \tTrain Accuracy: 80.00\n",
            "Epoch: 0/10 \tIteration: 500 \t\tTrain Loss: 0.419 \tTrain Accuracy: 82.00\n",
            "\tTrain Loss: 0.490 \tTrain Acc: 75.000\n",
            "\t\tVal Loss: 0.404 \t\tVal Acc: 82.330\n",
            "Time to train epoch: 20.12274169921875 s\n",
            "Epoch: 1/10 \tIteration: 600 \t\tTrain Loss: 0.360 \tTrain Accuracy: 88.00\n",
            "Epoch: 1/10 \tIteration: 700 \t\tTrain Loss: 0.304 \tTrain Accuracy: 88.00\n",
            "Epoch: 1/10 \tIteration: 800 \t\tTrain Loss: 0.375 \tTrain Accuracy: 84.00\n",
            "Epoch: 1/10 \tIteration: 900 \t\tTrain Loss: 0.271 \tTrain Accuracy: 88.00\n",
            "Epoch: 1/10 \tIteration: 1000 \t\tTrain Loss: 0.357 \tTrain Accuracy: 80.00\n",
            "Epoch: 1/10 \tIteration: 1100 \t\tTrain Loss: 0.298 \tTrain Accuracy: 86.00\n",
            "\tTrain Loss: 0.337 \tTrain Acc: 85.320\n",
            "\t\tVal Loss: 0.341 \t\tVal Acc: 85.670\n",
            "Time to train epoch: 20.478940725326538 s\n",
            "Epoch: 2/10 \tIteration: 1200 \t\tTrain Loss: 0.225 \tTrain Accuracy: 88.00\n",
            "Epoch: 2/10 \tIteration: 1300 \t\tTrain Loss: 0.233 \tTrain Accuracy: 92.00\n",
            "Epoch: 2/10 \tIteration: 1400 \t\tTrain Loss: 0.310 \tTrain Accuracy: 86.00\n",
            "Epoch: 2/10 \tIteration: 1500 \t\tTrain Loss: 0.219 \tTrain Accuracy: 90.00\n",
            "Epoch: 2/10 \tIteration: 1600 \t\tTrain Loss: 0.271 \tTrain Accuracy: 88.00\n",
            "Epoch: 2/10 \tIteration: 1700 \t\tTrain Loss: 0.213 \tTrain Accuracy: 96.00\n",
            "\tTrain Loss: 0.272 \tTrain Acc: 88.797\n",
            "\t\tVal Loss: 0.312 \t\tVal Acc: 86.970\n",
            "Time to train epoch: 21.346391677856445 s\n",
            "Epoch: 3/10 \tIteration: 1800 \t\tTrain Loss: 0.156 \tTrain Accuracy: 96.00\n",
            "Epoch: 3/10 \tIteration: 1900 \t\tTrain Loss: 0.194 \tTrain Accuracy: 92.00\n",
            "Epoch: 3/10 \tIteration: 2000 \t\tTrain Loss: 0.254 \tTrain Accuracy: 88.00\n",
            "Epoch: 3/10 \tIteration: 2100 \t\tTrain Loss: 0.179 \tTrain Accuracy: 94.00\n",
            "Epoch: 3/10 \tIteration: 2200 \t\tTrain Loss: 0.197 \tTrain Accuracy: 94.00\n",
            "Epoch: 3/10 \tIteration: 2300 \t\tTrain Loss: 0.159 \tTrain Accuracy: 96.00\n",
            "\tTrain Loss: 0.226 \tTrain Acc: 91.013\n",
            "\t\tVal Loss: 0.302 \t\tVal Acc: 87.240\n",
            "Time to train epoch: 20.3861026763916 s\n",
            "Epoch: 4/10 \tIteration: 2400 \t\tTrain Loss: 0.121 \tTrain Accuracy: 96.00\n",
            "Epoch: 4/10 \tIteration: 2500 \t\tTrain Loss: 0.164 \tTrain Accuracy: 96.00\n",
            "Epoch: 4/10 \tIteration: 2600 \t\tTrain Loss: 0.179 \tTrain Accuracy: 90.00\n",
            "Epoch: 4/10 \tIteration: 2700 \t\tTrain Loss: 0.132 \tTrain Accuracy: 96.00\n",
            "Epoch: 4/10 \tIteration: 2800 \t\tTrain Loss: 0.158 \tTrain Accuracy: 96.00\n",
            "Epoch: 4/10 \tIteration: 2900 \t\tTrain Loss: 0.132 \tTrain Accuracy: 94.00\n",
            "\tTrain Loss: 0.184 \tTrain Acc: 93.063\n",
            "\t\tVal Loss: 0.312 \t\tVal Acc: 87.250\n",
            "Time to train epoch: 20.32174515724182 s\n",
            "Epoch: 5/10 \tIteration: 3000 \t\tTrain Loss: 0.105 \tTrain Accuracy: 96.00\n",
            "Epoch: 5/10 \tIteration: 3100 \t\tTrain Loss: 0.137 \tTrain Accuracy: 96.00\n",
            "Epoch: 5/10 \tIteration: 3200 \t\tTrain Loss: 0.138 \tTrain Accuracy: 92.00\n",
            "Epoch: 5/10 \tIteration: 3300 \t\tTrain Loss: 0.108 \tTrain Accuracy: 96.00\n",
            "Epoch: 5/10 \tIteration: 3400 \t\tTrain Loss: 0.092 \tTrain Accuracy: 98.00\n",
            "Epoch: 5/10 \tIteration: 3500 \t\tTrain Loss: 0.131 \tTrain Accuracy: 94.00\n",
            "\tTrain Loss: 0.150 \tTrain Acc: 94.577\n",
            "\t\tVal Loss: 0.332 \t\tVal Acc: 87.100\n",
            "Time to train epoch: 20.37988805770874 s\n",
            "Epoch: 6/10 \tIteration: 3600 \t\tTrain Loss: 0.098 \tTrain Accuracy: 98.00\n",
            "Epoch: 6/10 \tIteration: 3700 \t\tTrain Loss: 0.093 \tTrain Accuracy: 98.00\n",
            "Epoch: 6/10 \tIteration: 3800 \t\tTrain Loss: 0.142 \tTrain Accuracy: 96.00\n",
            "Epoch: 6/10 \tIteration: 3900 \t\tTrain Loss: 0.111 \tTrain Accuracy: 92.00\n",
            "Epoch: 6/10 \tIteration: 4000 \t\tTrain Loss: 0.050 \tTrain Accuracy: 98.00\n",
            "Epoch: 6/10 \tIteration: 4100 \t\tTrain Loss: 0.105 \tTrain Accuracy: 94.00\n",
            "\tTrain Loss: 0.134 \tTrain Acc: 94.777\n",
            "\t\tVal Loss: 0.356 \t\tVal Acc: 86.560\n",
            "Time to train epoch: 20.46888279914856 s\n",
            "Epoch: 7/10 \tIteration: 4200 \t\tTrain Loss: 0.088 \tTrain Accuracy: 96.00\n",
            "Epoch: 7/10 \tIteration: 4300 \t\tTrain Loss: 0.103 \tTrain Accuracy: 94.00\n",
            "Epoch: 7/10 \tIteration: 4400 \t\tTrain Loss: 0.081 \tTrain Accuracy: 96.00\n",
            "Epoch: 7/10 \tIteration: 4500 \t\tTrain Loss: 0.108 \tTrain Accuracy: 94.00\n",
            "Epoch: 7/10 \tIteration: 4600 \t\tTrain Loss: 0.036 \tTrain Accuracy: 98.00\n",
            "Epoch: 7/10 \tIteration: 4700 \t\tTrain Loss: 0.046 \tTrain Accuracy: 98.00\n",
            "\tTrain Loss: 0.103 \tTrain Acc: 96.157\n",
            "\t\tVal Loss: 0.492 \t\tVal Acc: 83.820\n",
            "Time to train epoch: 20.494580507278442 s\n",
            "Epoch: 8/10 \tIteration: 4800 \t\tTrain Loss: 0.115 \tTrain Accuracy: 96.00\n",
            "Epoch: 8/10 \tIteration: 4900 \t\tTrain Loss: 0.057 \tTrain Accuracy: 98.00\n",
            "Epoch: 8/10 \tIteration: 5000 \t\tTrain Loss: 0.039 \tTrain Accuracy: 100.00\n",
            "Epoch: 8/10 \tIteration: 5100 \t\tTrain Loss: 0.052 \tTrain Accuracy: 98.00\n",
            "Epoch: 8/10 \tIteration: 5200 \t\tTrain Loss: 0.052 \tTrain Accuracy: 98.00\n",
            "Epoch: 8/10 \tIteration: 5300 \t\tTrain Loss: 0.039 \tTrain Accuracy: 100.00\n",
            "\tTrain Loss: 0.072 \tTrain Acc: 97.480\n",
            "\t\tVal Loss: 0.577 \t\tVal Acc: 83.650\n",
            "Time to train epoch: 20.50251817703247 s\n",
            "Epoch: 9/10 \tIteration: 5400 \t\tTrain Loss: 0.029 \tTrain Accuracy: 100.00\n",
            "Epoch: 9/10 \tIteration: 5500 \t\tTrain Loss: 0.056 \tTrain Accuracy: 98.00\n",
            "Epoch: 9/10 \tIteration: 5600 \t\tTrain Loss: 0.028 \tTrain Accuracy: 100.00\n",
            "Epoch: 9/10 \tIteration: 5700 \t\tTrain Loss: 0.039 \tTrain Accuracy: 98.00\n",
            "Epoch: 9/10 \tIteration: 5800 \t\tTrain Loss: 0.022 \tTrain Accuracy: 100.00\n",
            "Epoch: 9/10 \tIteration: 5900 \t\tTrain Loss: 0.065 \tTrain Accuracy: 98.00\n",
            "\tTrain Loss: 0.062 \tTrain Acc: 97.757\n",
            "\t\tVal Loss: 0.537 \t\tVal Acc: 86.080\n",
            "Time to train epoch: 20.5458881855011 s\n",
            "Test Loss: 0.559 \tTest Acc: 85.650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),\"./lstm-cnn-imdb.pth\")"
      ],
      "metadata": {
        "id": "z4Tjh75SWd8c"
      },
      "execution_count": 57,
      "outputs": []
    }
  ]
}