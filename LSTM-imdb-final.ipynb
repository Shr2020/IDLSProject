{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir ./data\n",
    "# !wget -O ./data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -zxf ./data/aclImdb_v1.tar.gz -C ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='./data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    # TODO: try out SnowBallStemmer\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"./cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word in word_count.keys():\n",
    "                word_count[word] = word_count[word] + 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "   \n",
    "    sorted_words = {k: v for k, v in sorted(word_count.items(), key=lambda item: item[1], reverse=True)}\n",
    "    sorted_words = [word for word in sorted_words.keys()]\n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict\n",
    "\n",
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = preprocess_data()\n",
    "type(train_X), type(train_y)\n",
    "train_X.extend(test_X)\n",
    "train_y.extend(test_y)\n",
    "len(train_X), len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "type(train_X), type(train_y)\n",
    "temp = list(zip(train_X, train_y))\n",
    "random.shuffle(temp)\n",
    "train_X, train_y = zip(*temp)\n",
    "# res1 and res2 come out as tuples, and so must be converted to lists.\n",
    "train_X, train_y = list(train_X), list(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "type(train_X), type(train_y)\n",
    "print(len(train_X))\n",
    "train_list = train_X[:30000]\n",
    "train_labels_list = train_y[:30000]\n",
    "test_list = train_X[30000:]\n",
    "test_labels_list = train_y[30000:]\n",
    "print(len(train_list), len(train_labels_list), len(test_list), len(test_labels_list))\n",
    "print(train_list[0], test_list[0], train_labels_list[0], test_labels_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dict(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/pytorch' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict_qrnn.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_X_len = convert_and_pad_data(word_dict, train_list)\n",
    "test_X, test_X_len = convert_and_pad_data(word_dict, test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.shape, len(train_X_len))\n",
    "print(test_X.shape, len(test_X_len))\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.concat([pd.DataFrame(train_labels_list), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1).to_csv(os.path.join(data_dir, 'train_qrnn.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(test_labels_list), pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1).to_csv(os.path.join(data_dir, 'test_qrnn.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If Files present start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "data_dir = './data/pytorch'\n",
    "with open(os.path.join(data_dir, 'word_dict_qrnn.pkl'), \"rb\") as f:\n",
    "    word_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 502) (20000, 502)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "train = pd.read_csv(os.path.join(data_dir, 'train_qrnn.csv'), header=None, names=None)\n",
    "test_sample = pd.read_csv(os.path.join(data_dir, 'test_qrnn.csv'), header=None, names=None)\n",
    "print(train.shape, test_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000, 502), (10000, 502), (10000, 502))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test, val = train_test_split(test_sample, test_size=0.5)\n",
    "train.shape, test.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_y = torch.from_numpy(train[[0]].values).float().squeeze()\n",
    "train_X = torch.from_numpy(train.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "train_ds = torch.utils.data.TensorDataset(train_X, train_y)\n",
    "# Build the dataloader\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=50)\n",
    "\n",
    "######val data\n",
    "# Turn the input pandas dataframe into tensors\n",
    "val_y = torch.from_numpy(val[[0]].values).float().squeeze()\n",
    "val_X = torch.from_numpy(val.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "val_ds = torch.utils.data.TensorDataset(val_X, val_y)\n",
    "# Build the dataloader\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=50)\n",
    "\n",
    "\n",
    "#### Test data\n",
    "# Turn the input pandas dataframe into tensors\n",
    "test_y = torch.from_numpy(test[[0]].values).float().squeeze()\n",
    "test_X = torch.from_numpy(test.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "test_ds = torch.utils.data.TensorDataset(test_X, test_y)\n",
    "# Build the dataloader\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=50)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the simple RNN model we will be using to perform Sentiment Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the model by settingg up the various layers.\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dense = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        self.word_dict = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input.\n",
    "        \"\"\"\n",
    "        x = x.t()\n",
    "        lengths = x[0,:]\n",
    "        reviews = x[1:,:]\n",
    "        embeds = self.embedding(reviews)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = self.dense(lstm_out)\n",
    "        out = out[lengths - 1, range(len(lengths))]\n",
    "        return self.sig(out.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file.\n",
    "filename = \"LSTM_imdb.csv\"\n",
    "def write_to_csv(epochs, train_loss, train_acc, val_loss, val_acc, time_train):\n",
    "    epoch = [i for i in range(epochs)]\n",
    "    df_metrics = pd.DataFrame(list(zip(epoch, train_loss, train_acc, val_loss, val_acc, time_train)), columns =['Epoch', 'train_loss', 'train_acc', 'val_loss', 'val_acc', 'train_time'])\n",
    "    df_metrics.to_csv(filename, index=False)\n",
    "    \n",
    "def append_to_csv(epochs, accuracy):\n",
    "    acc = [accuracy for i in range(epochs)]\n",
    "    df_csv = pd.read_csv(filename)\n",
    "    df_csv['Test_Accuracy']  = acc\n",
    "    df_csv.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(model, train_dl, val_dl, epochs, optimizer, loss_fn, device):\n",
    "    train_loss_epoch = []\n",
    "    train_acc_epoch = []\n",
    "    val_loss_epoch = []\n",
    "    val_accuracy_epoch = []\n",
    "    time_train = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "        train_acc = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for batch in train_dl:         \n",
    "            batch_X, batch_y = batch\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch_X)\n",
    "            loss = loss_fn(prediction, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            result = np.round(prediction.detach().cpu())\n",
    "            total_loss += loss.data.item()\n",
    "            total += batch_y.size(0)\n",
    "            correct += (result == batch_y.cpu()).sum().item()\n",
    "            train_acc = correct/total\n",
    "        train_loss_epoch.append(np.round(total_loss / len(train_dl), 3))\n",
    "        train_acc_epoch.append(np.round(train_acc*100,3))\n",
    "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_dl)))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            val_loss = []\n",
    "            for inputs, labels in val_dl:\n",
    "                inputs_val, labels_val = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                prediction = model(inputs_val)\n",
    "                loss = loss_fn(prediction, labels_val)\n",
    "                val_loss.append(np.round(loss.item(),3))\n",
    "                result = np.round(prediction.cpu())\n",
    "                total += labels_val.size(0)\n",
    "                correct += (result == labels_val.cpu()).sum().item()\n",
    "            val_accuracy_epoch.append(np.round((correct/total)*100, 3))\n",
    "            val_loss_epoch.append(np.round(np.mean(val_loss),3))\n",
    "            end = time.time() - start\n",
    "            print(\"Val Loss: {:.3f}\".format(np.mean(val_loss)), \"\\tVal Acc: {:.3f}\".format(correct/total))\n",
    "            time_train.append(np.round(end,3))\n",
    "    write_to_csv(epochs, train_loss_epoch, train_acc_epoch, val_loss_epoch, val_accuracy_epoch, time_train)\n",
    "    return model\n",
    "\n",
    "def test(model, test_dl, epochs):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "#     results = []\n",
    "#     labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:         \n",
    "            batch_X, batch_y = batch\n",
    "            batch_X = batch_X.to(device)\n",
    "            prediction = model(batch_X)\n",
    "            result = np.round(prediction.cpu())\n",
    "#             results.extend(list(result.numpy()))\n",
    "#             labels.extend(list(batch_y.numpy()))\n",
    "            total += batch_y.size(0)\n",
    "            correct += (result == batch_y).sum().item()\n",
    "    acc = correct/total\n",
    "    append_to_csv(epochs, np.round(acc*100, 3))\n",
    "    print(\"Accuracy:\", (correct/total)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, BCELoss: 0.601867962082227\n",
      "Val Loss: 0.487 \tVal Acc: 0.768\n",
      "Epoch: 1, BCELoss: 0.41929418275753655\n",
      "Val Loss: 0.399 \tVal Acc: 0.829\n",
      "Epoch: 2, BCELoss: 0.34020929778615633\n",
      "Val Loss: 0.357 \tVal Acc: 0.847\n",
      "Epoch: 3, BCELoss: 0.35414703994989394\n",
      "Val Loss: 0.397 \tVal Acc: 0.829\n",
      "Epoch: 4, BCELoss: 0.30965810439238944\n",
      "Val Loss: 0.320 \tVal Acc: 0.865\n",
      "Epoch: 5, BCELoss: 0.28246908616274596\n",
      "Val Loss: 0.321 \tVal Acc: 0.869\n",
      "Epoch: 6, BCELoss: 0.24996221296489238\n",
      "Val Loss: 0.326 \tVal Acc: 0.871\n",
      "Epoch: 7, BCELoss: 0.239388638548553\n",
      "Val Loss: 0.320 \tVal Acc: 0.877\n",
      "Epoch: 8, BCELoss: 0.22275431678940852\n",
      "Val Loss: 0.328 \tVal Acc: 0.880\n",
      "Epoch: 9, BCELoss: 0.211343239210546\n",
      "Val Loss: 0.384 \tVal Acc: 0.867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding): Embedding(5000, 32, padding_idx=0)\n",
       "  (lstm): LSTM(32, 100)\n",
       "  (dense): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(32, 100, 5000).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "train(model, train_dl, val_dl, epochs, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.13\n"
     ]
    }
   ],
   "source": [
    "test(model, test_dl, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env2_gpu",
   "language": "python",
   "name": "my_env2_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
