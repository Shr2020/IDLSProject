{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm-amazon-hyperband.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import sys\n",
        "import os\n",
        "prefix = '/content/gdrive/My Drive/'\n",
        "# modify \"customized_path_to_your_homework\" here to where you uploaded your homework\n",
        "customized_path_to_your_homework = 'IDLSProject-main'\n",
        "sys_path = os.path.join(prefix, customized_path_to_your_homework)\n",
        "sys.path.append(sys_path)"
      ],
      "metadata": {
        "id": "0NGnI0V2aJKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/gdrive/My Drive/IDLSProject-main'"
      ],
      "metadata": {
        "id": "maz2GPgMaLa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VivgYY1oaBKZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "data_dir = './data/pytorch'\n",
        "with open(os.path.join(data_dir, 'word_dict_amazon.pkl'), \"rb\") as f:\n",
        "    word_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "train = pd.read_csv(os.path.join(data_dir, 'train_amazon.csv'), header=None, names=None)\n",
        "test_sample = pd.read_csv(os.path.join(data_dir, 'test_amazon.csv'), header=None, names=None)\n",
        "print(train.shape, test_sample.shape)"
      ],
      "metadata": {
        "id": "QlxOq5leaFtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "test, val = train_test_split(test_sample, test_size=0.5)\n",
        "train.shape, test.shape, val.shape"
      ],
      "metadata": {
        "id": "ieV6MbC9aVls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "# Turn the input pandas dataframe into tensors\n",
        "train_y = torch.from_numpy(train[[0]].values).float().squeeze()\n",
        "train_X = torch.from_numpy(train.drop([0], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "train_ds = torch.utils.data.TensorDataset(train_X, train_y)\n",
        "# Build the dataloader\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=50)\n",
        "\n",
        "######val data\n",
        "# Turn the input pandas dataframe into tensors\n",
        "val_y = torch.from_numpy(val[[0]].values).float().squeeze()\n",
        "val_X = torch.from_numpy(val.drop([0], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "val_ds = torch.utils.data.TensorDataset(val_X, val_y)\n",
        "# Build the dataloader\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=50)\n",
        "\n",
        "\n",
        "#### Test data\n",
        "# Turn the input pandas dataframe into tensors\n",
        "test_y = torch.from_numpy(test[[0]].values).float().squeeze()\n",
        "test_X = torch.from_numpy(test.drop([0], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "test_ds = torch.utils.data.TensorDataset(test_X, test_y)\n",
        "# Build the dataloader\n",
        "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=50)\n",
        "print(test_y.shape)"
      ],
      "metadata": {
        "id": "CST5I9yaaX_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10"
      ],
      "metadata": {
        "id": "7fxCwrH00-IP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    This is the simple RNN model we will be using to perform Sentiment Analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,trial,vocab_size):\n",
        "        \"\"\"\n",
        "        Initialize the model by settingg up the various layers.\n",
        "        \"\"\"\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        dropout = trial.suggest_uniform(\"dropout\",0.1, 0.6)\n",
        "        hidden_dim = trial.suggest_int(\"hidden_dim\",16,256)\n",
        "        embedding_dim = trial.suggest_int(\"embedding_dim\",16,128)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.dense = nn.Linear(in_features=hidden_dim, out_features=1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "        self.word_dict = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input.\n",
        "        \"\"\"\n",
        "        x = x.t()\n",
        "        lengths = x[0,:]\n",
        "        reviews = x[1:,:]\n",
        "        embeds = self.embedding(reviews)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        out = self.dense(lstm_out)\n",
        "        out = out[lengths - 1, range(len(lengths))]\n",
        "        return self.sig(out.squeeze())"
      ],
      "metadata": {
        "id": "xaQD_YGZabxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Define an objective function to be minimized.\n",
        "def objective(trial):\n",
        "  optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "  #epochs = trial.suggest_int(\"epochs\",5,15)\n",
        "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-0)\n",
        "  #momentum = trial.suggest_uniform(\"momentum\", 0.0, 1.0)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = LSTMClassifier(trial,5000).to(device)\n",
        "  trial.set_user_attr(key=\"best_model\", value=model)\n",
        "  optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "  loss_fn = torch.nn.BCELoss()\n",
        "  val_accuracy = 0\n",
        "  for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        hidden = None\n",
        "        for batch in train_dl:         \n",
        "            batch_X, batch_y = batch\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            prediction = model(batch_X)\n",
        "            loss = loss_fn(prediction, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.data.item()\n",
        "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_dl)))\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            val_loss = []\n",
        "            for inputs, labels in val_dl:\n",
        "                inputs_val, labels_val = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                prediction = model(inputs_val)\n",
        "                loss = loss_fn(prediction, labels_val)\n",
        "                val_loss.append(loss.item())\n",
        "                result = np.round(prediction.cpu())\n",
        "                total += labels_val.size(0)\n",
        "                correct += (result == labels_val.cpu()).sum().item()\n",
        "            val_accuracy_epoch.append(correct/total)\n",
        "            val_loss_epoch.append(np.mean(val_loss))\n",
        "            val_accuracy = correct/total\n",
        "            end = time.time() - start\n",
        "            print(\"Val Loss: {:.3f}\".format(np.mean(val_loss)), \"\\tVal Acc: {:.3f}\".format(correct/total))\n",
        "        time_train.append(end)\n",
        "    write_to_csv(epochs, train_loss, val_loss_epoch, val_accuracy_epoch, time_train)\n",
        "  return val_accuracy"
      ],
      "metadata": {
        "id": "8tcyawYhoxJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def callback(study, trial):\n",
        "    if study.best_trial.number == trial.number:\n",
        "        study.set_user_attr(key=\"best_model\", value=trial.user_attrs[\"best_model\"])"
      ],
      "metadata": {
        "id": "H7glGgaD0LJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\",pruner=optuna.pruners.HyperbandPruner(\n",
        "        min_resource=1, max_resource=epochs, reduction_factor=3\n",
        "    ),)\n",
        "study.optimize(objective, n_trials=10,callbacks=[callback])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "best_model=study.user_attrs[\"best_model\"]\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "  print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "j3FbxeJ4o6uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_dl, epochs):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "#     results = []\n",
        "#     labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dl:         \n",
        "            batch_X, batch_y = batch\n",
        "            batch_X = batch_X.to(device)\n",
        "            prediction = model(batch_X)\n",
        "            result = np.round(prediction.cpu())\n",
        "#             results.extend(list(result.numpy()))\n",
        "#             labels.extend(list(batch_y.numpy()))\n",
        "            total += batch_y.size(0)\n",
        "            correct += (result == batch_y).sum().item()\n",
        "    acc = correct/total\n",
        "    append_to_csv(epochs, correct/total)\n",
        "    print(\"Accuracy:\", correct/total)"
      ],
      "metadata": {
        "id": "CnKuMqG60ncI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(best_model, test_dl, epochs)"
      ],
      "metadata": {
        "id": "lTxep3bI0zaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write to file.\n",
        "filename = \"LSTM_amazon.csv\"\n",
        "def write_to_csv(epochs, train_loss, val_loss, val_acc, time_train):\n",
        "    epoch = [i for i in range(epochs)]\n",
        "    df_metrics = pd.DataFrame(list(zip(epoch, train_loss, val_loss, val_acc, time_train)), columns =['Epoch', 'train_loss', 'val_loss', 'val_acc', 'train_time'])\n",
        "    df_metrics.to_csv(filename)\n",
        "def append_to_csv(epochs, accuracy):\n",
        "    acc = [accuracy for i in range(epochs)]\n",
        "    df_csv = pd.read_csv(file_name)\n",
        "    df_csv['Test_Accuracy']  = accuracy\n",
        "    df_metrics.to_csv(filename)"
      ],
      "metadata": {
        "id": "n1wI4W7kagMH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}